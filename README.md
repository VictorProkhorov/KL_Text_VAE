# On the Importance of the Kullback-Leibler Divergence Term in Variational Autoencoders for Text Generation

## Citing

If you find this material useful in your research, please cite:

```
@InProceedings{prokhorov_etal:WNGT2019,
  author={Victor Prokhorov and Ehsan Shareghi and Yingzhen Li and  Mohammad T. Pilehvar and Nigel Collier},
  title={On the Importance of the Kullback-Leibler Divergence Term in Variational Autoencoders for Text Generation},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
  year={2019},
  month={November},
  address={Hong Kong},
}  
```

## Licence

The code in this repository is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License version 3 as published by the Free Software Foundation. The code is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the [GNU General Public License](https://www.gnu.org/licenses/gpl-3.0.en.html) for more details.


## Contact info

For questions or more information please use the following:
* **Email:** vp361@cam.ac.uk 

